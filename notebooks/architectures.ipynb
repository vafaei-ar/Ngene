{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(x,scope,filters=12,kernel_size=5,norm=True,avtive=True):\n",
    "    \n",
    "    y = tf.layers.conv2d(x, filters=filters,\n",
    "                            kernel_size=kernel_size,\n",
    "                            padding='SAME',\n",
    "                            kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.05),\n",
    "                            bias_initializer=tf.constant_initializer(0.1),\n",
    "                            name=scope+'_conv')\n",
    "    if norm:    y = tf.layers.batch_normalization(y,name=scope+'_norm')\n",
    "    if avtive:    y = tf.nn.relu(y,name=scope+'_act')\n",
    "    \n",
    "    return y\n",
    "\n",
    "def architecture(x_in,n_layers,n_channel=1,res=3):\n",
    "    \n",
    "    layers = [x_in]\n",
    "    for i in range(n_layers-1):\n",
    "        layers.append(conv_layer(layers[-1],scope='layer_'+str(i+1)))\n",
    "        print(layers[-1])\n",
    "        \n",
    "        if res:\n",
    "            if (((i-1)%res==0) & (i>1)):\n",
    "                layers.append(layers[-1]+layers[i-res])\n",
    "                print('Res layer',i,'+',i-res,':')\n",
    "                print(layers[-1])\n",
    "        \n",
    "    layers.append(conv_layer(layers[-1],scope='layer_'+str(n_layers),filters=n_channel,avtive=0))\n",
    "    print(layers[-1])\n",
    "\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"layer_1_act:0\", shape=(?, 20, 20, 12), dtype=float32)\n",
      "Tensor(\"layer_2_act:0\", shape=(?, 20, 20, 12), dtype=float32)\n",
      "Tensor(\"layer_3_act:0\", shape=(?, 20, 20, 12), dtype=float32)\n",
      "Tensor(\"layer_4_act:0\", shape=(?, 20, 20, 12), dtype=float32)\n",
      "Res layer 3 + 1 :\n",
      "Tensor(\"add:0\", shape=(?, 20, 20, 12), dtype=float32)\n",
      "Tensor(\"layer_5_norm/FusedBatchNorm:0\", shape=(?, 20, 20, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x_in = tf.placeholder(tf.float32,[None,20,20,3])\n",
    "layers = architecture(x_in,5,res=2)\n",
    "x_out = layers[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semi U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x,num_outputs,kernel_size,stride,padding='VALID',activation_fn=tf.nn.relu,trainable=True,scope=None):\n",
    "    return tf.contrib.layers.conv2d(\n",
    "        x,\n",
    "        num_outputs=num_outputs,\n",
    "        kernel_size=kernel_size,\n",
    "        stride=stride,\n",
    "        padding=padding,\n",
    "        data_format='NHWC',\n",
    "        rate=1,\n",
    "        activation_fn=activation_fn,\n",
    "        normalizer_fn=None,\n",
    "        normalizer_params=None,\n",
    "        weights_regularizer=None,\n",
    "        biases_initializer=tf.constant_initializer(0.1),\n",
    "        biases_regularizer=None,\n",
    "        reuse=None,\n",
    "        variables_collections=None,\n",
    "        outputs_collections=None,\n",
    "        trainable=trainable,\n",
    "        scope=scope\n",
    "    )\n",
    "\n",
    "def deconv2d(x,num_outputs,kernel_size,stride,padding='VALID',activation_fn=tf.nn.relu,trainable=True,scope=None):\n",
    "    return tf.contrib.layers.conv2d_transpose(\n",
    "        x,\n",
    "        num_outputs=num_outputs,\n",
    "        kernel_size=kernel_size,\n",
    "        stride=stride,\n",
    "        padding=padding,\n",
    "        data_format='NHWC',\n",
    "        activation_fn=activation_fn,\n",
    "        normalizer_fn=None,\n",
    "        normalizer_params=None,\n",
    "        weights_regularizer=None,\n",
    "        biases_initializer=tf.constant_initializer(0.1),\n",
    "        biases_regularizer=None,\n",
    "        reuse=None,\n",
    "        variables_collections=None,\n",
    "        outputs_collections=None,\n",
    "        trainable=trainable,\n",
    "        scope=scope\n",
    "    )\n",
    "\n",
    "def architecture(x_in,drop_out_rate=0.3):\n",
    "\n",
    "#    x = tf.placeholder(dtype=tf.float32, shape=(20,l,l,5))\n",
    "    x = x_in\n",
    "    l = x.get_shape().as_list()[1]\n",
    "    np2 = int(np.log(l)/np.log(2))\n",
    "    main = 2**np2\n",
    "    n_residue = l-main\n",
    "    res = []\n",
    "    nt = 1\n",
    "    while n_residue>np.sum(res)-len(res):\n",
    "        res.append(nt)\n",
    "        nt += 8\n",
    "    res = res[1:-1]\n",
    "    res.append(n_residue-np.sum(res)+len(res)+1)\n",
    "        \n",
    "    print('===================================================')\n",
    "    print('===================== ENCODER =====================')\n",
    "    print('===================================================')\n",
    "    print(x)\n",
    "    layers = OrderedDict()\n",
    "    nr = len(res)\n",
    "    i_layer = 0\n",
    "    n_layers = 0\n",
    "    for i in range(nr):\n",
    "        i_layer += 1\n",
    "        ks = res[nr-i-1]\n",
    "        x = conv2d(x,6,[ks,ks],[1,1],padding='VALID',scope='encode/deconv_'+str(i_layer))\n",
    "        x = tf.layers.batch_normalization(x,name='encode/batch_'+str(i_layer))\n",
    "        x = tf.layers.dropout(x,rate=drop_out_rate,training=True,name='encode/dropout_'+str(i_layer))\n",
    "        x = tf.nn.relu(x,name='encode/relu_'+str(i_layer))\n",
    "        layers['encode_'+str(i_layer)] = x\n",
    "        print(x)\n",
    "        \n",
    "    for i in range(np2-5):\n",
    "        i_layer += 1\n",
    "        x = conv2d(x,6,[5,5],[2,2],padding='SAME',scope='encode/deconv_'+str(i_layer))\n",
    "        x = tf.layers.batch_normalization(x,name='encode/batch_'+str(i_layer))\n",
    "        x = tf.layers.dropout(x,rate=drop_out_rate,training=True,name='encode/dropout_'+str(i_layer))\n",
    "        x = tf.nn.relu(x,name='encode/relu_'+str(i_layer))\n",
    "        layers['encode_'+str(i_layer)] = x\n",
    "        print(x)\n",
    "    n_layers = i_layer\n",
    "        \n",
    "    print('')\n",
    "    print('===================================================')\n",
    "    print('===================== DECODER =====================')\n",
    "    print('===================================================')\n",
    "\n",
    "    i_layer = 0\n",
    "    for i in range(np2-5):\n",
    "        i_layer += 1\n",
    "        x = deconv2d(x,6,[5,5],[2,2],padding='SAME',scope='decode/deconv_'+str(i_layer))\n",
    "        x = tf.layers.batch_normalization(x,name='decode/batch_'+str(i_layer))\n",
    "        x = tf.layers.dropout(x,rate=drop_out_rate,training=True,name='decode/dropout_'+str(i_layer))\n",
    "        x = tf.nn.relu(x,name='decode/relu_'+str(i_layer))\n",
    "        layers['decode_'+str(i_layer)] = x\n",
    "        print(x)\n",
    "\n",
    "    for i in range(nr):\n",
    "        i_layer += 1\n",
    "        ks = res[i]\n",
    "        x = deconv2d(x,6,[ks,ks],[1,1],padding='VALID',scope='decode/deconv_'+str(i_layer))\n",
    "        x = tf.layers.batch_normalization(x,name='decode/batch_'+str(i_layer))\n",
    "        x = tf.layers.dropout(x,rate=drop_out_rate,training=True,name='decode/dropout_'+str(i_layer))\n",
    "        x = tf.nn.relu(x,name='decode/relu_'+str(i_layer))\n",
    "        layers['decode_'+str(i_layer)] = x\n",
    "        print(x)\n",
    "    \n",
    "    return [layers,x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================\n",
      "===================== ENCODER =====================\n",
      "===================================================\n",
      "Tensor(\"Placeholder:0\", shape=(?, 400, 400, 3), dtype=float32)\n",
      "Tensor(\"encode/relu_1:0\", shape=(?, 376, 376, 6), dtype=float32)\n",
      "Tensor(\"encode/relu_2:0\", shape=(?, 336, 336, 6), dtype=float32)\n",
      "Tensor(\"encode/relu_3:0\", shape=(?, 304, 304, 6), dtype=float32)\n",
      "Tensor(\"encode/relu_4:0\", shape=(?, 280, 280, 6), dtype=float32)\n",
      "Tensor(\"encode/relu_5:0\", shape=(?, 264, 264, 6), dtype=float32)\n",
      "Tensor(\"encode/relu_6:0\", shape=(?, 256, 256, 6), dtype=float32)\n",
      "Tensor(\"encode/relu_7:0\", shape=(?, 128, 128, 6), dtype=float32)\n",
      "Tensor(\"encode/relu_8:0\", shape=(?, 64, 64, 6), dtype=float32)\n",
      "Tensor(\"encode/relu_9:0\", shape=(?, 32, 32, 6), dtype=float32)\n",
      "\n",
      "===================================================\n",
      "===================== DECODER =====================\n",
      "===================================================\n",
      "Tensor(\"decode/relu_1:0\", shape=(?, 64, 64, 6), dtype=float32)\n",
      "Tensor(\"decode/relu_2:0\", shape=(?, 128, 128, 6), dtype=float32)\n",
      "Tensor(\"decode/relu_3:0\", shape=(?, 256, 256, 6), dtype=float32)\n",
      "Tensor(\"decode/relu_4:0\", shape=(?, 264, 264, 6), dtype=float32)\n",
      "Tensor(\"decode/relu_5:0\", shape=(?, 280, 280, 6), dtype=float32)\n",
      "Tensor(\"decode/relu_6:0\", shape=(?, 304, 304, 6), dtype=float32)\n",
      "Tensor(\"decode/relu_7:0\", shape=(?, 336, 336, 6), dtype=float32)\n",
      "Tensor(\"decode/relu_8:0\", shape=(?, 376, 376, 6), dtype=float32)\n",
      "Tensor(\"decode/relu_9:0\", shape=(?, 400, 400, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x_in = tf.placeholder(tf.float32,[None,400,400,3])\n",
    "layers = architecture(x_in)\n",
    "x_out = layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
